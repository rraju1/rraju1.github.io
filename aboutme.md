---
layout: page
title: About me
---


I currently work at SambaNova Systems as a software engineer looking at improving large language models (LLMs). Currently, I am researching alignment methods to make language models comply with enterprise user preferences, which include techniques like RLHF, preference model pretraining (PMP), RLAIF, etc. Another aspect that I am working on is the viability of using State Space Models (SSMs) as an alternative to attention-based transformer models for language tasks on the SambaNova Systems software stack. I am especially interested making models more effiencent via sparsity either by reducing computational burden during training or inference with structured sparsity.

### Background

I obtained my PhD in computer engineering from UW-Madison under the supervision of Dr. Mikko Lipasti. My research interest is looking at computational efficiency aspects of deep learning. Some of my recent projects include: 1) Dynamic Data Pruning: we analyze how we can accelerate neural networks by dynamically removing subsets of the data while retaining performance and 2) PatchDrop for efficient training: we exploit the input representation of the vision transformer to drop patches in order to accelerate training. In my past internships, I have looked creating models which incur fewer operations which maintaining a similar accuracy to a computation-hungry baseline and accelerating the search phase of neural architecture search with saliency patches. Some of my past projects have looked at adversarial machine learning where we proposed BlurNet, which is a defense algorithm against the RP2 attack for stop sign classification. I graduated from MSOE in electrical engineering with background in firmware, embedded systems, and digital systems.
